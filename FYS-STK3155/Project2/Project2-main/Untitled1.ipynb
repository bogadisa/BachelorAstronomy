{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e75f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scratch import gradient_decent\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Class neuralnetowrk, array of layers. Backprop function. Feed forward function.\n",
    "#Class layer, stores weights in matrix. Stores index of layer, activation function.\n",
    "\n",
    "\n",
    "#Backprop, computing the gradients.\n",
    "\n",
    "#Two classes\n",
    "\n",
    "#what is intrinsic to layer?\n",
    "#wegihts and amount of nodes, bias\n",
    "#so these are determined when a layer is initiated\n",
    "#each node is also dependent on amount of features\n",
    "#what about final layer?\n",
    "#determined by categories\n",
    "#how many categories?\n",
    "#is it a layer in the traditional sense?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_nodes, n_features, activation_func, func_derivative, bias=0.1, weights=None):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_features = n_features\n",
    "        if weights == None:\n",
    "            self.initialize_weights()\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        if isinstance(bias, float):\n",
    "            self.initialize_bias(bias)\n",
    "        else:\n",
    "            self.bias = bias\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.func_derivative = func_derivative\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.randn(self.n_features, self.n_nodes)\n",
    "\n",
    "    def initialize_bias(self, bias):\n",
    "        self.bias = np.zeros(self.n_nodes) + bias\n",
    "\n",
    "    @property\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "    @get_bias.setter\n",
    "    def get_bias(self, bias):\n",
    "        self.bias = bias\n",
    "    \n",
    "    @property\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    @get_weights.setter\n",
    "    def get_weights(self, weights):\n",
    "        self.weights = weights\n",
    "    \n",
    "    @property\n",
    "    def get_n_nodes(self):\n",
    "        return self.n_nodes\n",
    "\n",
    "    @get_n_nodes.setter\n",
    "    def get_n_nodes(self, n_nodes):\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "    def get_z(self, a):\n",
    "        weights = self.get_weights\n",
    "        bias = self.get_bias\n",
    "        print(np.shape(bias), np.shape(weights), np.shape(a))\n",
    "        z = np.matmul(a, weights) + bias\n",
    "        return z\n",
    "\n",
    "    def y(self, z):\n",
    "        self.y = self.activation_func(z)\n",
    "        return self.y\n",
    "\n",
    "    @property\n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "\n",
    "    def get_u(self, y):\n",
    "        weights = self.get_weights\n",
    "        bias = self.get_bias\n",
    "\n",
    "        u = np.matmul(y, weights) + bias\n",
    "\n",
    "        return u\n",
    "\n",
    "    @property\n",
    "    def error(self):\n",
    "        return self.error\n",
    "\n",
    "    @error.setter\n",
    "    def error(self, error):\n",
    "        self.error = error\n",
    "\n",
    "\n",
    "\n",
    "#NN?\n",
    "#amount of hidden layers\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, X_data, y_data, n_layers, n_nodes, n_categories, activation_funcs, funcs_derivative, epochs=10, batch_size=100, eta=0.1, lmbd=0):\n",
    "        self.X_data_full = X_data\n",
    "        self.y_data_full = y_data\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        if len(X_data.shape) == 2:\n",
    "            self.n_features = X_data.shape[1]\n",
    "        else:\n",
    "            self.n_features = 1\n",
    "        #print(self.n_features)\n",
    "\n",
    "        self.layers = [Layer(n_nodes, self.n_features, activation_funcs, funcs_derivative)]\n",
    "        if isinstance(n_nodes, int):\n",
    "            for i in range(1, n_layers+1):\n",
    "                self.layers.append(Layer(n_nodes, self.layers[i-1].get_n_nodes, activation_funcs, funcs_derivative))\n",
    "        else:\n",
    "            for i in n_nodes:\n",
    "                self.layers.append(Layer(i, self.layers[i-1].get_n_nodes, activation_funcs, funcs_derivative))\n",
    "\n",
    "        #output layer\n",
    "        self.layers.append(Layer(n_categories, self.layers[i].get_n_nodes, activation_funcs, funcs_derivative))\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "    def feed_forward(self, X_data):\n",
    "\n",
    "        #input layer\n",
    "        inputLayer = self.layers[0]\n",
    "        z1 = inputLayer.get_z(X_data)\n",
    "\n",
    "        #y = np.zeros((len(self.layers), z1.shape[0], z1.shape[1]))\n",
    "        y = [inputLayer.y(z1)]\n",
    "\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            u = layer.get_u(y[i])\n",
    "            print(np.shape(u))\n",
    "            \n",
    "            y.append(layer.y(u))\n",
    "\n",
    "        self.y = y\n",
    "\n",
    "    def backProp(self, y_data):\n",
    "        #(100, 10) (100,)\n",
    "        #print(np.shape(self.y[-1]), np.shape(y_data))\n",
    "        error_output = self.y[-1] - y_data\n",
    "        self.layers[-1].error = error_output\n",
    "        prevLayer = self.layers[-1]\n",
    "        #-2 due to input and output layer, maybe it should be just -1 to include input layer\n",
    "        for i in range(self.n_layers - 2): \n",
    "            layer = self.layers[-2-i]\n",
    "            nextLayer = self.layers[-3-i]\n",
    "            layer.error = np.matmul(prevLayer.error, prevLayer.get_weights)*layer.derivative(layer.get_z)\n",
    "            layer.get_weights = layer.get_weights - self.eta*layer.error*nextLayer.get_y\n",
    "            layer.get_bias = layer.get_bias - self.eta*layer.error\n",
    "            prevLayer = layer\n",
    "\n",
    "    def train(self):\n",
    "        data_incides = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                chosen_datapoints = np.random.choice(data_incides, size=self.batch_size, replace=False)\n",
    "                #print(np.shape(chosen_datapoints))\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.y_data = self.y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward(self.X_data)\n",
    "                self.backProp(self.y_data)\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "\n",
    "        #input layer\n",
    "        inputLayer = self.layers[0]\n",
    "        z1 = inputLayer.get_z(X)\n",
    "\n",
    "        #y = np.zeros((len(self.layers), z1.shape[0], z1.shape[1]))\n",
    "        y = [inputLayer.y(z1)]\n",
    "\n",
    "        for i, layer in enumerate(self.layers[1:]):\n",
    "            u = layer.get(y[i])\n",
    "            y.append(layer.y(u))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.feed_forward_out(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cbf6484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (1, 100) (100,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19564\\3566842499.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_deriv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mtest_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19564\\3107667891.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_data_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchosen_datapoints\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19564\\3107667891.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(self, X_data)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m#input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0minputLayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputLayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;31m#y = np.zeros((len(self.layers), z1.shape[0], z1.shape[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19564\\3107667891.py\u001b[0m in \u001b[0;36mget_z\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 100)"
     ]
    }
   ],
   "source": [
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(x):\n",
    "        sig_x  = sigmoid(x)\n",
    "        return sig_x*(1 - sig_x)\n",
    "\n",
    "    def f(x):\n",
    "        return 1 + 3*x + 5*x**2 + 6*x**3\n",
    "\n",
    "    n = 1000\n",
    "    x = np.linspace(0, 1, n)\n",
    "    y = f(x)\n",
    "\n",
    "    X = np.c_[np.ones((n,1)), x, x**2, x**3]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y,test_size=1/4)\n",
    "\n",
    "    dnn = NeuralNetwork(X_train, Y_train, 3, 100, 1, sigmoid, sigmoid_deriv)\n",
    "    dnn.train()\n",
    "    test_predict = dnn.predict(X_test)\n",
    "\n",
    "    plt.scatter(X_test, Y_test, label=\"Actual\")\n",
    "    plt.scatter(X_test, test_predict, label=\"Model\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
